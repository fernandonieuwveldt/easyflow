{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building Pipeline using easyflow feature_encoders module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is a fusion between keras layers and tensorflow feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from easyflow.data.mapper import TensorflowDataMapper\n",
    "from easyflow.feature_encoders.transformer import FeatureColumnTransformer, FeatureUnionTransformer\n",
    "from easyflow.feature_encoders.feature_encoder import NumericalFeatureEncoder, EmbeddingFeatureEncoder, CategoricalFeatureEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 14)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "data_frame = pd.read_csv(data_url, header=None, names=CSV_HEADER)\n",
    "labels = data_frame.pop(\"income_bracket\")\n",
    "labels_binary = 1.0 * (labels == \" >50K\")\n",
    "\n",
    "print(f\"Train dataset shape: {data_frame.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "dataset_mapper = TensorflowDataMapper() \n",
    "dataset = dataset_mapper.map(data_frame, labels_binary)\n",
    "\n",
    "train_data_set, val_data_set = dataset_mapper.split_data_set(dataset)\n",
    "train_data_set = train_data_set.batch(batch_size)\n",
    "val_data_set = val_data_set.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_NAMES = [\n",
    "    \"workclass\",\n",
    "    \"marital_status\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\"]\n",
    "\n",
    "EMBEDDING_FEATURES_NAMES = ['education',\n",
    "                            'occupation',\n",
    "                            'native_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder_list = [('numerical_features', NumericalFeatureEncoder(), NUMERIC_FEATURE_NAMES),\n",
    "                        ('categorical_features', CategoricalFeatureEncoder(), CATEGORICAL_FEATURES_NAMES),\n",
    "                        ('embedding_features_deep', EmbeddingFeatureEncoder(), EMBEDDING_FEATURES_NAMES),\n",
    "                        ('embedding_features_wide', CategoricalFeatureEncoder(), EMBEDDING_FEATURES_NAMES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up feature layer and feature encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main column transformer classes namely FeatureColumnTransformer and FeatureUnionTransformer. For this example we are going to build a Wide and Deep model architecture. So we will be using the FeatureColumnTransformer since it gives us more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer_inputs, feature_encoders =  FeatureColumnTransformer(feature_encoder_list).transform(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_features = feature_encoders['numerical_features']+\\\n",
    "                feature_encoders['categorical_features']+\\\n",
    "                feature_encoders['embedding_features_deep']\n",
    "\n",
    "wide_features = feature_encoders['embedding_features_wide']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep = tf.keras.layers.DenseFeatures(deep_features)(feature_layer_inputs)\n",
    "deep = tf.keras.layers.BatchNormalization()(deep)\n",
    "\n",
    "wide = tf.keras.layers.DenseFeatures(wide_features)(feature_layer_inputs)\n",
    "\n",
    "for nodes in [128, 64, 32]:\n",
    "    deep = tf.keras.layers.Dense(nodes, activation='relu')(deep)\n",
    "    deep = tf.keras.layers.Dropout(0.5)(deep)\n",
    "\n",
    "wide_and_deep = tf.keras.layers.concatenate([deep, wide])\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(wide_and_deep)\n",
    "model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=output)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['fnlwgt'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 2s 16ms/step - loss: 0.4937 - accuracy: 0.7495 - auc: 0.7289 - val_loss: 0.5570 - val_accuracy: 0.6604 - val_auc: 0.8338\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3894 - accuracy: 0.8161 - auc: 0.8585 - val_loss: 0.4023 - val_accuracy: 0.7849 - val_auc: 0.8948\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3665 - accuracy: 0.8296 - auc: 0.8764 - val_loss: 0.3414 - val_accuracy: 0.8401 - val_auc: 0.9052\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3534 - accuracy: 0.8419 - auc: 0.8855 - val_loss: 0.3259 - val_accuracy: 0.8493 - val_auc: 0.9079\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3486 - accuracy: 0.8422 - auc: 0.8894 - val_loss: 0.3234 - val_accuracy: 0.8484 - val_auc: 0.9093\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3438 - accuracy: 0.8434 - auc: 0.8925 - val_loss: 0.3190 - val_accuracy: 0.8498 - val_auc: 0.9104\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3379 - accuracy: 0.8475 - auc: 0.8960 - val_loss: 0.3164 - val_accuracy: 0.8514 - val_auc: 0.9122\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3347 - accuracy: 0.8494 - auc: 0.8975 - val_loss: 0.3157 - val_accuracy: 0.8515 - val_auc: 0.9121\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 0.3324 - accuracy: 0.8490 - auc: 0.8994 - val_loss: 0.3162 - val_accuracy: 0.8528 - val_auc: 0.9118\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.3273 - accuracy: 0.8493 - auc: 0.9029 - val_loss: 0.3129 - val_accuracy: 0.8536 - val_auc: 0.9134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb30596160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_set, validation_data=val_data_set, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
