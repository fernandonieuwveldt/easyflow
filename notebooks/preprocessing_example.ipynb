{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Encoder, Pipeline, SequentialEncoder and FeatureUnion example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easyflow.preprocessing module contains functionality similar to what sklearn does with its Pipeline, FeatureUnion and ColumnTransformer does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization, CategoryEncoding, StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from easyflow.data.mapper import TensorflowDataMapper\n",
    "from easyflow.preprocessing.preprocessor import Encoder, Pipeline, SequentialEncoder, FeatureUnion\n",
    "from easyflow.preprocessing.custom import IdentityPreprocessingLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and map as tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the TensorflowDataMapper class to map pandas data frame to a tf.data.Dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "dataframe = pd.read_csv(file_url)\n",
    "dataframe = dataframe.copy()\n",
    "labels = dataframe.pop(\"target\")\n",
    "\n",
    "batch_size = 32\n",
    "dataset_mapper = TensorflowDataMapper() \n",
    "dataset = dataset_mapper.map(dataframe, labels)\n",
    "train_data_set, val_data_set = dataset_mapper.split_data_set(dataset)\n",
    "train_data_set = train_data_set.batch(batch_size)\n",
    "val_data_set = val_data_set.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope']\n",
    "CATEGORICAL_FEATURES = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'ca']\n",
    "# thal is represented as a string\n",
    "STRING_CATEGORICAL_FEATURES = ['thal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Preprocessing layer using FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Encoder and SequentialEncoder to preprocess features by putting everything in a FeatureUnion object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder_list = [\n",
    "                        Encoder([('numeric_encoder', Normalization, NUMERICAL_FEATURES)]),\n",
    "                        Encoder([('categorical_encoder', CategoryEncoding, CATEGORICAL_FEATURES)]),\n",
    "                        # For feature thal we first need to run StringLookup followed by a CategoryEncoding layer\n",
    "                        SequentialEncoder([('string_encoder', StringLookup, STRING_CATEGORICAL_FEATURES),\n",
    "                                           ('categorical_encoder', CategoryEncoding, STRING_CATEGORICAL_FEATURES)])\n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = FeatureUnion(feature_encoder_list)\n",
    "all_feature_inputs, preprocessing_layer = encoder.encode(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_1/concat:0\", shape=(None, 31), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5836 - accuracy: 0.7137 - auc: 0.6482 - val_loss: 0.4946 - val_accuracy: 0.7500 - val_auc: 0.8295\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4902 - accuracy: 0.7489 - auc: 0.7854 - val_loss: 0.4647 - val_accuracy: 0.7763 - val_auc: 0.8847\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4588 - accuracy: 0.7621 - auc: 0.8468 - val_loss: 0.3977 - val_accuracy: 0.7763 - val_auc: 0.9414\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4305 - accuracy: 0.8106 - auc: 0.8523 - val_loss: 0.3642 - val_accuracy: 0.8421 - val_auc: 0.9382\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4080 - accuracy: 0.7930 - auc: 0.8773 - val_loss: 0.3285 - val_accuracy: 0.8816 - val_auc: 0.9580\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4089 - accuracy: 0.8062 - auc: 0.8794 - val_loss: 0.3875 - val_accuracy: 0.8553 - val_auc: 0.9373\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3725 - accuracy: 0.8194 - auc: 0.8920 - val_loss: 0.3476 - val_accuracy: 0.8684 - val_auc: 0.9165\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8238 - auc: 0.8917 - val_loss: 0.3494 - val_accuracy: 0.8289 - val_auc: 0.9454\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3437 - accuracy: 0.8502 - auc: 0.9161 - val_loss: 0.3668 - val_accuracy: 0.8289 - val_auc: 0.9079\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.8414 - auc: 0.9073 - val_loss: 0.2732 - val_accuracy: 0.8816 - val_auc: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# setup simple network\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(preprocessing_layer)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=all_feature_inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), tf.keras.metrics.AUC(name='auc')])\n",
    "history=model.fit(train_data_set, validation_data=val_data_set, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexibility with Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion subclasses Pipeline and concatenates(i.e union) the layers. For more flexibility like a wide and deep neural network, Pipeline class will give you more flexibility in that sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder_list = [\n",
    "                        Encoder([('numeric_encoder', Normalization, NUMERICAL_FEATURES)]),\n",
    "                        Encoder([('categorical_encoder', CategoryEncoding, CATEGORICAL_FEATURES)]),\n",
    "                        # For feature thal we first need to run StringLookup followed by a CategoryEncoding layer\n",
    "                        SequentialEncoder([('string_encoder', StringLookup, STRING_CATEGORICAL_FEATURES),\n",
    "                                           ('categorical_encoder', CategoryEncoding, STRING_CATEGORICAL_FEATURES)])\n",
    "                        ]\n",
    "\n",
    "encoder = Pipeline(feature_encoder_list)\n",
    "all_feature_inputs1, preprocessing_layer1 = encoder.encode(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'normalization_43/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'normalization_44/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'normalization_45/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'normalization_46/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'normalization_47/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'normalization_48/truediv:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'category_encoding_31/bincount/DenseBincount:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'category_encoding_32/bincount/DenseBincount:0' shape=(None, 5) dtype=float32>, <tf.Tensor 'category_encoding_33/bincount/DenseBincount:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'category_encoding_34/bincount/DenseBincount:0' shape=(None, 3) dtype=float32>, <tf.Tensor 'category_encoding_35/bincount/DenseBincount:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'category_encoding_36/bincount/DenseBincount:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'category_encoding_37/bincount/DenseBincount:0' shape=(None, 7) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
