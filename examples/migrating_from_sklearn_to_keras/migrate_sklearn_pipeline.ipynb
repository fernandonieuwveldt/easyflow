{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Migrating SKLearn Pipeline to Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"sklearn_to_tensorflow.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "*Image source: https://medium.com/towards-data-science/from-scikit-learn-to-tensorflow-part-1-9ee0b96d4c85*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we look at ways to migrate an Sklearn training pipeline to Tensorflow Keras. There might be a few reasons to move from Sklearn to Tensorflow.\n",
    "\n",
    "### Possible benefits:\n",
    "* Flexibility to basically any ML model architecture.\n",
    "* Distributed training utilising GPUs.\n",
    "* Flexibility when it comes to retraining.\n",
    "* Tensorflow serving.\n",
    "* etc.\n",
    "\n",
    "### Objectives\n",
    "* We will implement a model training pipeline with Sklearn.\n",
    "* Implement the same pipeline using only Tensorflow Keras modules. Preprocessing layers will used here.\n",
    "* Use EasyFlow for equivalent pipelines such as Sklearn’s Pipeline and ColumnTransformer module.\n",
    "\n",
    "Probably the most common use case comes from designing a ML pipeline where your preprocessing is implemented in Sklearn and model in Keras. We will have different artifacts for each part. With Keras and the `EasyFlow` module it will be easy to implement a native Keras solution.\n",
    "\n",
    "For our dataset we will use the popular heart dataset. It consists of a mix of feature types such as numerical, categorical feature encoded as int’s and as strings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    import easyflow\n",
    "except:\n",
    "    ! pip install easy-tensorflow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "dataframe = pd.read_csv(file_url)\n",
    "labels = dataframe.pop(\"target\")\n",
    "\n",
    "NUMERICAL_FEATURES = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'age']\n",
    "CATEGORICAL_FEATURES = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'ca']\n",
    "# thal is represented as a string\n",
    "STRING_CATEGORICAL_FEATURES = ['thal']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sklearn Training Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), NUMERICAL_FEATURES),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_FEATURES+STRING_CATEGORICAL_FEATURES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "sklearn_model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocess),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "sklearn_model.fit(dataframe, labels)\n",
    "sklearn_model.score(dataframe, labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8613861386138614"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above Pipeline is not very involved and easy to implement. The idea here is to show how we can go about implementing an equivalent Pipeline in Tensorflow Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Migrate Pipeline to Tensorflow Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s create our feature preprocessing Pipeline natively in Keras. We will duplicate the same pipeline as implemented in the Sklearn section. Keras recently added preprocessing layers. The Keras equivalent preprocessors for StandardScaler and OneHotEncoder is Normalization and IntegerLookup respectively. When our categorical features are of type string we first need to apply StringLookup preprocessing layer followed by IntegerLookup layer.\n",
    "\n",
    "The last thing we need is a Keras implementation for Pipeline and ColumnTransformer. There is currently no implementation in Keras so we will use another package for this:\n",
    "\n",
    "**EasyFlow** : https://pypi.org/project/easy-tensorflow/\n",
    "pip install easy-tensorflow\n",
    "\n",
    "EasyFlow makes use of Keras preprocessing layers. All high level pipelines in EasyFlow such as FeatureUnion subclasses tf.keras.layers.Layer and thus behave like any other Keras layer. (FeatureUnion is equivalent to Sklearn’s ColumnTransformer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Normalization, IntegerLookup, StringLookup\n",
    "\n",
    "from easyflow.data.mapper import TensorflowDataMapper\n",
    "from easyflow.preprocessing.pipeline import FeatureUnion\n",
    "from easyflow.preprocessing import FeatureInputLayer, StringToIntegerLookup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be making use of the Keras functional API. So we first need to create a feature input layer. Below we have a data type mapping dict as input to FeatureInputLayer. Next we will use FeatureUnion from EasyFlow module to implement similar preprocessing pipeline to ColumnTransformer. We will update the preprocessing layer states by running adapt method on our data. Keras preprocessing layers uses .*adapt* to update states and in this case similar to .*fit*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "feature_layer_inputs = FeatureInputLayer({\n",
    "    \"age\": tf.float32,\n",
    "    \"sex\": tf.float32,\n",
    "    \"cp\": tf.float32,\n",
    "    \"trestbps\": tf.float32,\n",
    "    \"chol\": tf.float32,\n",
    "    \"fbs\": tf.float32,\n",
    "    \"restecg\": tf.float32,\n",
    "    \"thalach\": tf.float32,\n",
    "    \"exang\": tf.float32,\n",
    "    \"oldpeak\": tf.float32,\n",
    "    \"slope\": tf.float32,\n",
    "    \"ca\": tf.float32,\n",
    "    \"thal\": tf.string\n",
    "})\n",
    "\n",
    "\n",
    "preprocessor = FeatureUnion(\n",
    "    feature_preprocessor_list = [\n",
    "        ('num', Normalization(), NUMERICAL_FEATURES),\n",
    "        ('cat', IntegerLookup(output_mode='binary'), CATEGORICAL_FEATURES),\n",
    "        ('str_cat', StringToIntegerLookup(), STRING_CATEGORICAL_FEATURES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# to update the states for preprocess layers:\n",
    "preprocessor.adapt(dataframe)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-04-23 18:13:57.646517: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As stated FeatureUnion and all other Pipelines in EasyFlow are Keras layers and are also callable. Below we setup our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "preprocessed_inputs = preprocessor(feature_layer_inputs)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(preprocessed_inputs)\n",
    "model = tf.keras.Model(inputs=feature_layer_inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ])\n",
    "\n",
    "model.fit(dict(dataframe), labels, batch_size=32, epochs=100, verbose=0)\n",
    "tf.keras.models.save_model(model=model, filepath='model')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\"model\")\n",
    "dict(zip(loaded_model.metrics_names, loaded_model.evaluate(dict(dataframe), labels)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n",
      "10/10 [==============================] - 0s 737us/step - loss: 0.3339 - accuracy: 0.8515 - auc: 0.9247\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 0.33392223715782166,\n",
       " 'accuracy': 0.8514851331710815,\n",
       " 'auc': 0.9247261881828308}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That is it! We successfully ported an Sklearn training pipeline to Tensorflow Keras by utilising preprocessing layers and EasyFlow’s feature preprocessing pipelines. We also persisted the model and loaded it for inference to showcase that it is truly end to end. The results compares well with our Sklearn implementation. Huge advantage here is the fact that preprocessing is part of the network and persisted as such. Without Keras preprocessing layers and EasyFlow’s pipeline implementation we usually had a separate Sklearn artifact for preprocessing. Using Sklearn for preprocessing and then feeding data to a Keras model used to be a common design pattern. With serving containers such as Tensorflow serving that can serve our models without python or Tensorflow installation makes this migration to a native Tensorflow Keras implementation very appealing. All you need a saved model.\n",
    "\n",
    "We can go one step further to improve our training speed. See next section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quick note on improving Keras training speed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will use the common pattern for training by creating a model that applies the preprocessing step to speed up training. When we start from raw data as in our example. We need to preprocess all preprocessing operations on the CPU and than feed that data to a GPU. Preprocessing is also not something that we train and it is independent from the forward pass. This will reduce our throughput as the GPU will be idle while waiting for data. To speed things up we will prefetch batches of preprocessed data. This will ensure that while we processing batch of data on the GPU the CPU is getting the next batch of preprocessed data ready.\n",
    "\n",
    "<img src=\"gpu_cpu_gaps.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "*Image taken from https://www.tensorflow.org*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# create our preprocessing model\n",
    "preprocessing_model = tf.keras.Model(feature_layer_inputs, preprocessed_inputs)\n",
    "\n",
    "# create training model that will be applied on the forward pass\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(preprocessed_inputs)\n",
    "training_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
    "training_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will map the Pandas dataframe to tensorflow.data.Datasets type. The preprocessing model above will be mapped onto our feature data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "batch_size = 32\n",
    "dataset_mapper = TensorflowDataMapper() \n",
    "dataset = dataset_mapper.map(dataframe, labels)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "preprocessed_ds = dataset.map(\n",
    "    lambda x, y: (preprocessing_model(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "training_model.fit(preprocessed_ds, batch_size=batch_size, epochs=100, verbose=0)\n",
    "\n",
    "# evaluate model\n",
    "dict(zip(training_model.metrics_names, training_model.evaluate(preprocessed_ds)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/10 [==============================] - 0s 467us/step - loss: 0.3193 - accuracy: 0.8713 - auc: 0.9275\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 0.3193400204181671,\n",
       " 'accuracy': 0.8712871074676514,\n",
       " 'auc': 0.927546501159668}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need one last step to create a model that can be used for inference. Since we splitted the model into a preprocessing and training step we can't save training_model as is. We need to plug the preprocessing back into the model. Lets create our inference model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "inference_model = tf.keras.Model(feature_layer_inputs, training_model(preprocessed_inputs))\n",
    "# compile model to get supplied metrics at inference time\n",
    "inference_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ])\n",
    "\n",
    "tf.keras.models.save_model(model=inference_model, filepath='saved_inference_model')\n",
    "saved_inference_model = tf.keras.models.load_model(\"saved_inference_model\")\n",
    "\n",
    "dict(zip(saved_inference_model.metrics_names, saved_inference_model.evaluate(dict(dataframe), labels)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_inference_model/assets\n",
      "10/10 [==============================] - 0s 770us/step - loss: 0.3193 - accuracy: 0.8713 - auc: 0.9275\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 0.3193400204181671,\n",
       " 'accuracy': 0.8712871074676514,\n",
       " 'auc': 0.927546501159668}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion we showed how we can migrate an sklearn training pipeline to Tensorflow and Keras. We started off by building our training pipeline in sklearn consisting of a preprocessing step and we used LogisticRegression as our estimator. We used the same sklearn pipeline and migrated it to Keras using preprocessing layers and easyflow Pipeline modules. Our model architecture was a simple linear model(Logistic Regression) with no hidden layers. Our feature preprocessing was part of our network architecture and we persisted and loaded the model to apply inference. We ended up with a final section on improving training speed by splitting the preprocessing and model steps for training. At the end we added the preprocessing back with the training model to create our inference model."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}